{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_0YYxVMxfbm"
      },
      "source": [
        "Installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYk-QZW0xoPT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deLMhcVsCP_x"
      },
      "source": [
        "Get the english dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H9vvUB11yoh"
      },
      "outputs": [],
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n",
        "!tar -xf MovieSummaries.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FpITMomcEoO"
      },
      "outputs": [],
      "source": [
        "!python3 -m nltk.downloader wordnet\n",
        "!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yenoksu1CYJX"
      },
      "source": [
        "Tokenize english dataset with NLTK and visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p-bwBo0928Bm",
        "outputId": "e2f9301b-4141-4ccf-b17d-4597a73859ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42303 entries, 0 to 42302\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      42303 non-null  int64 \n",
            " 1   text    42303 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 661.1+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: go\n",
            "id                                                              4018288\n",
            "text                  Debbie's favorite band is Dream Street, and he...\n",
            "tokenizedTextNLTK     [Debbie, favorite, band, Dream, Street, her, f...\n",
            "stemmedTextNLTK       [debbi, favorit, band, dream, street, her, fav...\n",
            "lemmatizedTextNLTK    [debbi, favorit, band, dream, street, her, fav...\n",
            "Name: 17, dtype: object\n",
            "id                                                              1480747\n",
            "text                  {{plot}} Following the sudden death of Kid's f...\n",
            "tokenizedTextNLTK     [plot, Following, sudden, death, Kid, father, ...\n",
            "stemmedTextNLTK       [plot, follow, sudden, death, kid, father, pop...\n",
            "lemmatizedTextNLTK    [plot, follow, sudden, death, kid, father, pop...\n",
            "Name: 14, dtype: object\n",
            "id                                                              1952976\n",
            "text                  {{plot}} The film opens in 1974, as a young gi...\n",
            "tokenizedTextNLTK     [plot, The, film, opens, 1974, young, girl, Da...\n",
            "stemmedTextNLTK       [plot, the, film, open, 1974, young, girl, dah...\n",
            "lemmatizedTextNLTK    [plot, the, film, open, 1974, young, girl, dah...\n",
            "Name: 6, dtype: object\n",
            "id                                                              1335380\n",
            "text                  The film is based on the events that happened ...\n",
            "tokenizedTextNLTK     [The, film, based, events, happened, ship, Exo...\n",
            "stemmedTextNLTK       [the, film, base, event, happen, ship, exodu, ...\n",
            "lemmatizedTextNLTK    [the, film, base, event, happen, ship, exodu, ...\n",
            "Name: 13, dtype: object\n",
            "id                                                             31186339\n",
            "text                  The nation of Panem consists of a wealthy Capi...\n",
            "tokenizedTextNLTK     [The, nation, Panem, consists, wealthy, Capito...\n",
            "stemmedTextNLTK       [the, nation, panem, consist, wealthi, capitol...\n",
            "lemmatizedTextNLTK    [the, nation, panem, consist, wealthi, capitol...\n",
            "Name: 1, dtype: object\n",
            "Enter your query: test\n",
            "id                                                             23890098\n",
            "text                  Shlykov, a hard-working taxi driver and Lyosha...\n",
            "tokenizedTextNLTK     [Shlykov, hard, working, taxi, driver, Lyosha,...\n",
            "stemmedTextNLTK       [shlykov, hard, work, taxi, driver, lyosha, sa...\n",
            "lemmatizedTextNLTK    [shlykov, hard, work, taxi, driver, lyosha, sa...\n",
            "Name: 0, dtype: object\n",
            "id                                                             31186339\n",
            "text                  The nation of Panem consists of a wealthy Capi...\n",
            "tokenizedTextNLTK     [The, nation, Panem, consists, wealthy, Capito...\n",
            "stemmedTextNLTK       [the, nation, panem, consist, wealthi, capitol...\n",
            "lemmatizedTextNLTK    [the, nation, panem, consist, wealthi, capitol...\n",
            "Name: 1, dtype: object\n",
            "id                                                             20663735\n",
            "text                  Poovalli Induchoodan  is sentenced for six yea...\n",
            "tokenizedTextNLTK     [Poovalli, Induchoodan, sentenced, six, years,...\n",
            "stemmedTextNLTK       [pooval, induchoodan, sentenc, six, year, pris...\n",
            "lemmatizedTextNLTK    [pooval, induchoodan, sentenc, six, year, pris...\n",
            "Name: 2, dtype: object\n",
            "id                                                              2231378\n",
            "text                  The Lemon Drop Kid , a New York City swindler,...\n",
            "tokenizedTextNLTK     [The, Lemon, Drop, Kid, New, York, City, swind...\n",
            "stemmedTextNLTK       [the, lemon, drop, kid, new, york, citi, swind...\n",
            "lemmatizedTextNLTK    [the, lemon, drop, kid, new, york, citi, swind...\n",
            "Name: 3, dtype: object\n",
            "id                                                               595909\n",
            "text                  Seventh-day Adventist Church pastor Michael Ch...\n",
            "tokenizedTextNLTK     [Seventh, day, Adventist, Church, pastor, Mich...\n",
            "stemmedTextNLTK       [seventh, day, adventist, church, pastor, mich...\n",
            "lemmatizedTextNLTK    [seventh, day, adventist, church, pastor, mich...\n",
            "Name: 4, dtype: object\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-475e48d173d5>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter your query: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;31m# tokenize query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# read as dataframe\n",
        "df_english = pd.read_csv('/content/MovieSummaries/plot_summaries.txt', delimiter = '\\t',names=['id','text'])\n",
        "df_english.info()\n",
        "df_english = df_english.head(20)\n",
        "\n",
        "# normalize and tokenize\n",
        "token_list_nltk = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for i in range(len(df_english)):\n",
        "  content = df_english['text'][i]\n",
        "  token_list_nltk.append(tokenizer.tokenize(content))\n",
        "\n",
        "# remove stop words\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for text_tokens in token_list_nltk:\n",
        "  for word in text_tokens:\n",
        "    if word in list(stop_words):\n",
        "      text_tokens.remove(word)\n",
        "\n",
        "# add to data frame\n",
        "df_english['tokenizedTextNLTK'] = token_list_nltk\n",
        "\n",
        "# stemming\n",
        "stemmer = PorterStemmer()\n",
        "stem_list_nltk = []\n",
        "for each_list in token_list_nltk:\n",
        "  each_list_stems = []\n",
        "  for word in each_list:\n",
        "    each_list_stems.append(stemmer.stem(word))\n",
        "  stem_list_nltk.append(each_list_stems)\n",
        "\n",
        "# add to data frame\n",
        "df_english['stemmedTextNLTK'] = stem_list_nltk\n",
        "\n",
        "# lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_list_nltk = []\n",
        "for each_list in stem_list_nltk:\n",
        "  each_list_lemmas = []\n",
        "  for word in each_list:\n",
        "    each_list_lemmas.append(lemmatizer.lemmatize(word))\n",
        "  lemma_list_nltk.append(each_list_lemmas)\n",
        "\n",
        "# add to data frame\n",
        "df_english['lemmatizedTextNLTK'] = lemma_list_nltk\n",
        "\n",
        "def count_list(lst, x):\n",
        "    count = 0\n",
        "    for i in range(len(lst)):\n",
        "        if x in lst[i]:\n",
        "            count+= 1\n",
        "    return count\n",
        "\n",
        "tf_dics_list = []\n",
        "all_tokens =[]\n",
        "for doc_tokens in lemma_list_nltk:\n",
        "  for token in doc_tokens:\n",
        "    all_tokens.append(token)\n",
        "\n",
        "all_tokens = set(all_tokens)\n",
        "for doc_tokens in lemma_list_nltk:\n",
        "  doc_dic_list = []\n",
        "  for token in all_tokens:\n",
        "    doc_dic_list.append(\n",
        "        {\n",
        "            'keyword':token,\n",
        "            'tf':(doc_tokens.count(token)/len(doc_tokens)),\n",
        "            'idf':0,\n",
        "            'tf-idf':0\n",
        "        }\n",
        "    )\n",
        "  tf_dics_list.append(doc_dic_list)\n",
        "\n",
        "for doc_dics in tf_dics_list:\n",
        "  for doc_dic in doc_dics:\n",
        "    doc_dic['idf'] = math.log(len(lemma_list_nltk)/count_list(lemma_list_nltk,doc_dic['keyword']))\n",
        "    doc_dic['tf-idf'] = doc_dic['tf'] * doc_dic['idf']\n",
        "\n",
        "while(True):\n",
        "\n",
        "  query = input('Enter your query: ')\n",
        "\n",
        "  # tokenize query\n",
        "  query_tokenized = []\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  query_tokenized = tokenizer.tokenize(query)\n",
        "\n",
        "  # remove stop words\n",
        "  query_removed_stop_words = []\n",
        "  for token in query_tokenized:\n",
        "     if token not in list(stop_words):\n",
        "        query_removed_stop_words.append(token)\n",
        "\n",
        "  # stem\n",
        "  query_stemmed = []\n",
        "  for token in query_removed_stop_words:\n",
        "      query_stemmed.append(stemmer.stem(token))\n",
        "\n",
        "  # lemma\n",
        "  query_lemmatized = []\n",
        "  for token in query_stemmed:\n",
        "      query_lemmatized.append(lemmatizer.lemmatize(token))\n",
        "\n",
        "  docs_scores = []\n",
        "  index_counter = 0\n",
        "  for doc in tf_dics_list:\n",
        "    doc_score = 0\n",
        "    for dic in doc:\n",
        "      if dic['keyword'] in query_lemmatized:\n",
        "        doc_score = doc_score + dic['tf-idf']\n",
        "    docs_scores.append({\n",
        "        'index':index_counter,\n",
        "        'score':doc_score\n",
        "    })\n",
        "    index_counter = index_counter + 1\n",
        "\n",
        "  newlist = sorted(docs_scores, key=itemgetter('score'), reverse=True)\n",
        "  for i in range(5):\n",
        "    print(df_english.loc[newlist[i]['index']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}