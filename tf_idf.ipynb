{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_0YYxVMxfbm"
      },
      "source": [
        "Installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYk-QZW0xoPT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deLMhcVsCP_x"
      },
      "source": [
        "Get the english dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H9vvUB11yoh"
      },
      "outputs": [],
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n",
        "!tar -xf MovieSummaries.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FpITMomcEoO"
      },
      "outputs": [],
      "source": [
        "!python3 -m nltk.downloader wordnet\n",
        "!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yenoksu1CYJX"
      },
      "source": [
        "Tokenize english dataset with NLTK and visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-bwBo0928Bm",
        "outputId": "8c2b8c79-7e8e-4ba2-d624-a1cb20ecd822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42303 entries, 0 to 42302\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      42303 non-null  int64 \n",
            " 1   text    42303 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 661.1+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: go\n",
            "id                                                              4018288\n",
            "text                  Debbie's favorite band is Dream Street, and he...\n",
            "tokenizedTextNLTK     [Debbie, favorite, band, Dream, Street, her, f...\n",
            "stemmedTextNLTK       [debbi, favorit, band, dream, street, her, fav...\n",
            "lemmatizedTextNLTK    [debbi, favorit, band, dream, street, her, fav...\n",
            "Name: 17, dtype: object\n",
            "id                                                              1480747\n",
            "text                  {{plot}} Following the sudden death of Kid's f...\n",
            "tokenizedTextNLTK     [plot, Following, sudden, death, Kid, father, ...\n",
            "stemmedTextNLTK       [plot, follow, sudden, death, kid, father, pop...\n",
            "lemmatizedTextNLTK    [plot, follow, sudden, death, kid, father, pop...\n",
            "Name: 14, dtype: object\n",
            "id                                                              1952976\n",
            "text                  {{plot}} The film opens in 1974, as a young gi...\n",
            "tokenizedTextNLTK     [plot, The, film, opens, 1974, young, girl, Da...\n",
            "stemmedTextNLTK       [plot, the, film, open, 1974, young, girl, dah...\n",
            "lemmatizedTextNLTK    [plot, the, film, open, 1974, young, girl, dah...\n",
            "Name: 6, dtype: object\n",
            "id                                                              1335380\n",
            "text                  The film is based on the events that happened ...\n",
            "tokenizedTextNLTK     [The, film, based, events, happened, ship, Exo...\n",
            "stemmedTextNLTK       [the, film, base, event, happen, ship, exodu, ...\n",
            "lemmatizedTextNLTK    [the, film, base, event, happen, ship, exodu, ...\n",
            "Name: 13, dtype: object\n",
            "id                                                             31186339\n",
            "text                  The nation of Panem consists of a wealthy Capi...\n",
            "tokenizedTextNLTK     [The, nation, Panem, consists, wealthy, Capito...\n",
            "stemmedTextNLTK       [the, nation, panem, consist, wealthi, capitol...\n",
            "lemmatizedTextNLTK    [the, nation, panem, consist, wealthi, capitol...\n",
            "Name: 1, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# read as dataframe\n",
        "df_english = pd.read_csv('/content/MovieSummaries/plot_summaries.txt', delimiter = '\\t',names=['id','text'])\n",
        "df_english.info()\n",
        "df_english = df_english.head(20)\n",
        "\n",
        "# normalize and tokenize\n",
        "token_list_nltk = []\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for i in range(len(df_english)):\n",
        "  content = df_english['text'][i]\n",
        "  token_list_nltk.append(tokenizer.tokenize(content))\n",
        "\n",
        "# remove stop words\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for text_tokens in token_list_nltk:\n",
        "  for word in text_tokens:\n",
        "    if word in list(stop_words):\n",
        "      text_tokens.remove(word)\n",
        "\n",
        "# add to data frame\n",
        "df_english['tokenizedTextNLTK'] = token_list_nltk\n",
        "\n",
        "# stemming\n",
        "stemmer = PorterStemmer()\n",
        "stem_list_nltk = []\n",
        "for each_list in token_list_nltk:\n",
        "  each_list_stems = []\n",
        "  for word in each_list:\n",
        "    each_list_stems.append(stemmer.stem(word))\n",
        "  stem_list_nltk.append(each_list_stems)\n",
        "\n",
        "# add to data frame\n",
        "df_english['stemmedTextNLTK'] = stem_list_nltk\n",
        "\n",
        "# lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_list_nltk = []\n",
        "for each_list in stem_list_nltk:\n",
        "  each_list_lemmas = []\n",
        "  for word in each_list:\n",
        "    each_list_lemmas.append(lemmatizer.lemmatize(word))\n",
        "  lemma_list_nltk.append(each_list_lemmas)\n",
        "\n",
        "# add to data frame\n",
        "df_english['lemmatizedTextNLTK'] = lemma_list_nltk\n",
        "\n",
        "def count_list(lst, x):\n",
        "    count = 0\n",
        "    for i in range(len(lst)):\n",
        "        if x in lst[i]:\n",
        "            count+= 1\n",
        "    return count\n",
        "\n",
        "tf_dics_list = []\n",
        "all_tokens =[]\n",
        "for doc_tokens in lemma_list_nltk:\n",
        "  for token in doc_tokens:\n",
        "    all_tokens.append(token)\n",
        "\n",
        "all_tokens = set(all_tokens)\n",
        "for doc_tokens in lemma_list_nltk:\n",
        "  doc_dic_list = []\n",
        "  for token in all_tokens:\n",
        "    doc_dic_list.append(\n",
        "        {\n",
        "            'keyword':token,\n",
        "            'tf':(doc_tokens.count(token)/len(doc_tokens)),\n",
        "            'idf':0,\n",
        "            'tf-idf':0\n",
        "        }\n",
        "    )\n",
        "  tf_dics_list.append(doc_dic_list)\n",
        "\n",
        "for doc_dics in tf_dics_list:\n",
        "  for doc_dic in doc_dics:\n",
        "    doc_dic['idf'] = math.log(len(lemma_list_nltk)/count_list(lemma_list_nltk,doc_dic['keyword']))\n",
        "    doc_dic['tf-idf'] = doc_dic['tf'] * doc_dic['idf']\n",
        "\n",
        "while(True):\n",
        "\n",
        "  query = input('Enter your query: ')\n",
        "\n",
        "  # tokenize query\n",
        "  query_tokenized = []\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  query_tokenized = tokenizer.tokenize(query)\n",
        "\n",
        "  # remove stop words\n",
        "  query_removed_stop_words = []\n",
        "  for token in query_tokenized:\n",
        "     if token not in list(stop_words):\n",
        "        query_removed_stop_words.append(token)\n",
        "\n",
        "  # stem\n",
        "  query_stemmed = []\n",
        "  for token in query_removed_stop_words:\n",
        "      query_stemmed.append(stemmer.stem(token))\n",
        "\n",
        "  # lemma\n",
        "  query_lemmatized = []\n",
        "  for token in query_stemmed:\n",
        "      query_lemmatized.append(lemmatizer.lemmatize(token))\n",
        "\n",
        "  docs_scores = []\n",
        "  index_counter = 0\n",
        "  for doc in tf_dics_list:\n",
        "    doc_score = 0\n",
        "    for dic in doc:\n",
        "      if dic['keyword'] in query_lemmatized:\n",
        "        doc_score = doc_score + dic['tf-idf']\n",
        "    docs_scores.append({\n",
        "        'index':index_counter,\n",
        "        'score':doc_score\n",
        "    })\n",
        "    index_counter = index_counter + 1\n",
        "\n",
        "  newlist = sorted(docs_scores, key=itemgetter('score'), reverse=True)\n",
        "  for i in range(5):\n",
        "    print(df_english.loc[newlist[i]['index']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}